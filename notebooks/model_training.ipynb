{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f847c2",
   "metadata": {},
   "source": [
    "# Sterility Classification model training notebook\n",
    "\n",
    "This notebook trains a binary classification model to determine instrument sterility using EfficientNet-B0 architecture.\n",
    "\n",
    "## Overview\n",
    "- **Task**: Binary classification (Sterile vs Non-sterile instruments)\n",
    "- **Architecture**: EfficientNet-B0 with ImageNet pre-training\n",
    "- **Dataset**: Images in `dataset/0/` (sterile) and `dataset/1/` (non-sterile)\n",
    "- **Features**: \n",
    "  - Data augmentation for training\n",
    "  - Optional blur filtering\n",
    "  - Class-balanced loss function\n",
    "  - Learning rate scheduling\n",
    "\n",
    "## Workflow\n",
    "1. Import dependencies and setup configuration\n",
    "2. Define utility functions and custom dataset\n",
    "3. Prepare data loaders with augmentation\n",
    "4. Setup model architecture and training components\n",
    "5. Train model with validation monitoring\n",
    "6. Evaluate final performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c32b28",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebd289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da3814",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_DIR = \"dataset\"          # –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å 0/ –∏ 1/\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 15\n",
    "    LR = 1e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    NUM_WORKERS = 4\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    IMG_SIZE = 224\n",
    "    REMOVE_BLURRY = False        # —É—Å—Ç–∞–Ω–æ–≤–∏ True, —á—Ç–æ–±—ã —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–º—ã—Ç—ã–µ\n",
    "    BLUR_THRESHOLD = 100.0       # –Ω–∏–∂–µ ‚Äî —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ä–∞–∑–º—ã—Ç—ã–º\n",
    "\n",
    "print(f\"Using device: {Config.DEVICE}\")\n",
    "print(f\"Dataset directory: {Config.DATA_DIR}\")\n",
    "print(f\"Image size: {Config.IMG_SIZE}\")\n",
    "print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
    "print(f\"Number of epochs: {Config.NUM_EPOCHS}\")\n",
    "print(f\"Learning rate: {Config.LR}\")\n",
    "print(f\"Remove blurry images: {Config.REMOVE_BLURRY}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91daa252",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd15798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_blurry(img_path, threshold=100.0):\n",
    "    \"\"\"\n",
    "    Check if an image is blurry using Laplacian variance.\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): Path to the image file\n",
    "        threshold (float): Threshold below which image is considered blurry\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if image is blurry, False otherwise\n",
    "    \"\"\"\n",
    "    image = cv2.imread(img_path)\n",
    "    if image is None:\n",
    "        return True\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return fm < threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bebb8e",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SterilityDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for sterility classification.\n",
    "    \n",
    "    Expected directory structure:\n",
    "    data_dir/\n",
    "    ‚îú‚îÄ‚îÄ 0/  # Sterile instruments\n",
    "    ‚îî‚îÄ‚îÄ 1/  # Non-sterile instruments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, transform=None, remove_blurry=False, blur_threshold=100.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Path to dataset directory\n",
    "            transform (callable): Optional transform to be applied to samples\n",
    "            remove_blurry (bool): Whether to filter out blurry images\n",
    "            blur_threshold (float): Threshold for blur detection\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        for class_id in [\"0\", \"1\"]:\n",
    "            class_dir = os.path.join(data_dir, class_id)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "                \n",
    "            for fname in os.listdir(class_dir):\n",
    "                fpath = os.path.join(class_dir, fname)\n",
    "                if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    continue\n",
    "                if remove_blurry and is_blurry(fpath, blur_threshold):\n",
    "                    continue\n",
    "                self.samples.append((fpath, int(class_id)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, float(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead98925",
   "metadata": {},
   "source": [
    "## 5. Data Preparation and Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42928ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Data transforms defined\")\n",
    "print(\"Training transforms: Resize, RandomFlip, ColorJitter, Rotation, Normalization\")\n",
    "print(\"Validation transforms: Resize, Normalization only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "full_dataset = SterilityDataset(\n",
    "    Config.DATA_DIR,\n",
    "    transform=train_transform,\n",
    "    remove_blurry=Config.REMOVE_BLURRY,\n",
    "    blur_threshold=Config.BLUR_THRESHOLD\n",
    ")\n",
    "\n",
    "# Split dataset into train/validation (80/20)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Apply validation transforms to validation dataset\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "print(\"Dataset loaded and split:\")\n",
    "print(f\"   Total samples: {len(full_dataset)}\")\n",
    "print(f\"   Training samples: {train_size}\")\n",
    "print(f\"   Validation samples: {val_size}\")\n",
    "\n",
    "# Check class distribution in the full dataset\n",
    "class_0_count = sum(1 for _, label in full_dataset.samples if label == 0)\n",
    "class_1_count = sum(1 for _, label in full_dataset.samples if label == 1)\n",
    "print(f\"   Class 0 (Sterile): {class_0_count}\")\n",
    "print(f\"   Class 1 (Non-sterile): {class_1_count}\")\n",
    "print(f\"   Class balance ratio: {class_1_count/class_0_count:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=Config.NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=Config.NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
    "print(f\"Number of workers: {Config.NUM_WORKERS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f35ab",
   "metadata": {},
   "source": [
    "## 6. Model Setup and Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1901df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model - EfficientNet-B0 with pre-trained weights\n",
    "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify classifier for binary classification (1 output neuron)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(Config.DEVICE)\n",
    "\n",
    "print(\"Model initialized:\")\n",
    "print(\"   Architecture: EfficientNet-B0\")\n",
    "print(\"   Pre-trained: ImageNet weights\")\n",
    "print(\"   Output neurons: 1 (binary classification)\")\n",
    "print(f\"   Device: {Config.DEVICE}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8031144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function with class balancing\n",
    "# Note: Using hard-coded values from original code - ideally should calculate from data\n",
    "n0, n1 = 2305, 2338  # Class counts: sterile, non-sterile\n",
    "\n",
    "# Increased positive weight to heavily penalize missing non-sterile cases\n",
    "# This makes the model more sensitive to non-sterile instruments\n",
    "# Higher weight = model pays more attention to not missing non-sterile cases\n",
    "pos_weight = torch.tensor([n0 / n1 * 3.0]).to(Config.DEVICE)  # 3x multiplier for safety\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Setup optimizer with weight decay\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=Config.LR, \n",
    "    weight_decay=Config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=Config.NUM_EPOCHS\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "TRAINING COMPONENTS CONFIGURATION\n",
    "\n",
    "Loss Function       : BCEWithLogitsLoss\n",
    "Positive Weight     : {pos_weight.item():.4f} (increased 3x for safety)\n",
    "Optimizer           : AdamW\n",
    "Learning Rate       : {Config.LR}\n",
    "Weight Decay        : {Config.WEIGHT_DECAY}\n",
    "LR Scheduler        : CosineAnnealingLR\n",
    "Decision Threshold  : 0.25 (conservative to catch all non-sterile cases)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f0c2a2",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36045a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m best_val_acc = \u001B[32m0.0\u001B[39m\n\u001B[32m      3\u001B[39m training_history = {\u001B[33m\"\u001B[39m\u001B[33mepoch\u001B[39m\u001B[33m\"\u001B[39m: [], \u001B[33m\"\u001B[39m\u001B[33mtrain_loss\u001B[39m\u001B[33m\"\u001B[39m: [], \u001B[33m\"\u001B[39m\u001B[33mval_acc\u001B[39m\u001B[33m\"\u001B[39m: []}\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33müöÄ Starting training for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mConfig\u001B[49m.NUM_EPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m epochs...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      6\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m60\u001B[39m)\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(Config.NUM_EPOCHS):\n\u001B[32m      9\u001B[39m     \u001B[38;5;66;03m# Training phase\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'Config' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop with Recall-focused metric\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "\n",
    "best_val_recall = 0.0\n",
    "DECISION_THRESHOLD = 0.25  # Lower threshold to be more sensitive to non-sterile cases\n",
    "training_history = {\n",
    "    \"epoch\": [], \n",
    "    \"train_loss\": [], \n",
    "    \"val_recall_class1\": [], \n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "print(f\"\"\"\n",
    "STARTING TRAINING\n",
    "\n",
    "Total Epochs        : {Config.NUM_EPOCHS}\n",
    "Primary Metric      : Recall for Class 1 (Non-sterile)\n",
    "Decision Threshold  : {DECISION_THRESHOLD} (conservative)\n",
    "\"\"\")\n",
    "\n",
    "for epoch in range(Config.NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 50 batches\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"Batch {batch_idx+1:4d}/{len(train_loader):4d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase with adjusted threshold\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "            outputs = model(images).squeeze()\n",
    "            # Use lower threshold (0.25) to be more conservative\n",
    "            preds = (torch.sigmoid(outputs) > DECISION_THRESHOLD).float()\n",
    "            # val_preds.extend(preds.cpu().numpy())\n",
    "            # val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().flatten().tolist())\n",
    "            val_labels.extend(labels.cpu().flatten().tolist())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    val_preds_np = np.array(val_preds)\n",
    "    val_labels_np = np.array(val_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_acc = (val_preds_np == val_labels_np).mean()\n",
    "    val_recall_class1 = recall_score(val_labels_np, val_preds_np, pos_label=1)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(val_labels_np, val_preds_np)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Save training history\n",
    "    training_history[\"epoch\"].append(epoch + 1)\n",
    "    training_history[\"train_loss\"].append(avg_train_loss)\n",
    "    training_history[\"val_recall_class1\"].append(val_recall_class1)\n",
    "    training_history[\"val_acc\"].append(val_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    saved_marker = \"\"\n",
    "    if val_recall_class1 > best_val_recall:\n",
    "        best_val_recall = val_recall_class1\n",
    "        torch.save(model.state_dict(), \"best_model_v0.0.1.pth\")\n",
    "        saved_marker = \" [MODEL SAVED]\"\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    print(f\"\"\"\n",
    "EPOCH {epoch+1}/{Config.NUM_EPOCHS}{saved_marker}\n",
    "\n",
    "Training Loss                : {avg_train_loss:.4f}\n",
    "Validation Accuracy          : {val_acc:.4f}\n",
    "Validation Recall (PRIMARY)  : {val_recall_class1:.4f}\n",
    "Current Learning Rate        : {current_lr:.6f}\n",
    "\n",
    "Confusion Matrix:\n",
    "                        Predicted\n",
    "                    Sterile    Non-sterile\n",
    "Actual Sterile      {tn:4d}       {fp:4d}\n",
    "       Non-sterile  {fn:4d}       {tp:4d}\n",
    "\n",
    "False Negatives (CRITICAL)   : {fn} (Non-sterile predicted as Sterile)\n",
    "False Positives              : {fp} (Sterile predicted as Non-sterile)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "TRAINING COMPLETED\n",
    "\n",
    "Best Recall (Non-sterile)    : {best_val_recall:.4f}\n",
    "Model saved as               : best_model_v0.0.1.pth\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57457307",
   "metadata": {},
   "source": [
    "## 8. Final Evaluation and Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation with detailed metrics\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "cm = confusion_matrix(val_labels_np, val_preds_np)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = precision_score(val_labels_np, val_preds_np)\n",
    "recall = recall_score(val_labels_np, val_preds_np)\n",
    "f1 = f1_score(val_labels_np, val_preds_np)\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(\n",
    "    val_labels_np, \n",
    "    val_preds_np, \n",
    "    target_names=[\"Sterile (0)\", \"Non-sterile (1)\"],\n",
    "    digits=4\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "FINAL VALIDATION METRICS\n",
    "\n",
    "Classification Report:\n",
    "{class_report}\n",
    "\n",
    "Confusion Matrix:\n",
    "                        Predicted\n",
    "                    Sterile    Non-sterile\n",
    "Actual Sterile      {tn:4d}       {fp:4d}\n",
    "       Non-sterile  {fn:4d}       {tp:4d}\n",
    "\n",
    "False Negatives (CRITICAL)   : {fn} (Non-sterile marked as Sterile)\n",
    "False Positives              : {fp} (Sterile marked as Non-sterile)\n",
    "\n",
    "Performance Metrics:\n",
    "Overall Accuracy             : {val_acc:.4f}\n",
    "Recall (Non-sterile)         : {recall:.4f}  [PRIMARY METRIC]\n",
    "Precision (Non-sterile)      : {precision:.4f}\n",
    "F1-Score                     : {f1:.4f}\n",
    "Decision Threshold           : {DECISION_THRESHOLD}\n",
    "\n",
    "Model Information:\n",
    "Model File                   : best_model_v0.0.1.pth\n",
    "Architecture                 : EfficientNet-B0\n",
    "Input Size                   : {Config.IMG_SIZE}x{Config.IMG_SIZE}\n",
    "Training Epochs              : {Config.NUM_EPOCHS}\n",
    "Best Recall (Non-sterile)    : {best_val_recall:.4f}\n",
    "Decision Threshold           : {DECISION_THRESHOLD} (conservative)\n",
    "Positive Class Weight        : 3x (penalizes missing non-sterile cases)\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
